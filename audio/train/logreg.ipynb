{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    NJU LAMDA\n",
    "    Video classification contest\n",
    "    fc -> sigm -> l2 loss\n",
    "    Author: Hao Zhang\n",
    "    Date: 2016.06.23\n",
    "    File: logreg.pynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "require('nn')\n",
    "require('optim')\n",
    "require('cutorch')\n",
    "require('cunn')\n",
    "require('gnuplot')\n",
    "\n",
    "torch.manualSeed(0)\n",
    "torch.setdefaulttensortype('torch.FloatTensor')\n",
    "dtype = 'torch.CudaTensor'\n",
    "\n",
    "opt = {\n",
    "    bs = 128,\n",
    "    m = 6000,\n",
    "    m_frame = 3059,\n",
    "    n = 26,\n",
    "    K = 5,\n",
    "    m_dev = 32,\n",
    "    m_train = 5500,\n",
    "    m_val = 500\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 6000\n",
       " 3059\n",
       "   26\n",
       "[torch.LongStorage of size 3]\n",
       "\n"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       " 6000\n",
       "    5\n",
       "[torch.LongStorage of size 2]\n",
       "\n"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.load('data.txt')\n",
    "labels = torch.load('labels.txt')\n",
    "\n",
    "print(data:size())\n",
    "print(labels:size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the Training Dataset and Move to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "local shuffled_ind = torch.randperm(opt.m):long()\n",
    "data = data:index(1, shuffled_ind):type(dtype)\n",
    "labels = labels:index(1, shuffled_ind):type(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Substraction and Std Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "local mean = data[{{1, opt.m_train}, {}, {}}]:mean()\n",
    "data:csub(mean)\n",
    "\n",
    "local std = data[{{1, opt.m_train}, {}, {}}]:std()\n",
    "data:div(std)\n",
    "\n",
    "local stat = {}\n",
    "stat.mean = mean\n",
    "stat.std = std\n",
    "torch.save('stat.bin', stat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split it into Train, Dev, and Val Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X:\t\n",
       " 5500\n",
       " 3059\n",
       "   26\n",
       "[torch.LongStorage of size 3]\n",
       "\n",
       "Y:\t\n",
       " 5500\n",
       "    5\n",
       "[torch.LongStorage of size 2]\n",
       "\n",
       "X_dev:\t\n",
       "   32\n",
       " 3059\n",
       "   26\n",
       "[torch.LongStorage of size 3]\n",
       "\n",
       "Y_dev:\t\n",
       " 32\n",
       "  5\n",
       "[torch.LongStorage of size 2]\n",
       "\n",
       "X_val:\t\n",
       "  500\n",
       " 3059\n",
       "   26\n",
       "[torch.LongStorage of size 3]\n",
       "\n",
       "Y_val:\t\n",
       " 500\n",
       "   5\n",
       "[torch.LongStorage of size 2]\n",
       "\n"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- Split the train, val, test set\n",
    "X = data[{{1, opt.m_train}, {}, {}}]\n",
    "Y = labels[{{1, opt.m_train}}]\n",
    "\n",
    "X_chunks = X:split(opt.bs, 1)\n",
    "Y_chunks = Y:split(opt.bs, 1)\n",
    "\n",
    "X_dev = data[{{1, opt.m_dev}, {}, {}}]\n",
    "Y_dev = labels[{{1, opt.m_dev}}]\n",
    "\n",
    "--X_chunks = X:split(opt.bs, 1)\n",
    "--y_chunks = y:split(opt.bs, 1)\n",
    "\n",
    "X_val = data[{{opt.m_train + 1, opt.m_train + opt.m_val}, {}, {}}]\n",
    "Y_val = labels[{{opt.m_train + 1, opt.m_train + opt.m_val}}]\n",
    "\n",
    "print('X:')\n",
    "print(X:size())\n",
    "print('Y:')\n",
    "print(Y:size())\n",
    "print('X_dev:')\n",
    "print(X_dev:size())\n",
    "print('Y_dev:')\n",
    "print(Y_dev:size())\n",
    "print('X_val:')\n",
    "print(X_val:size())\n",
    "print('Y_val:')\n",
    "print(Y_val:size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the Model and Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model and Move to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nn.Sequential {\n",
       "  [input -> (1) -> (2) -> (3) -> output]\n",
       "  (1): nn.Reshape(79534)\n",
       "  (2): nn.Linear(79534 -> 5)\n",
       "  (3): nn.Sigmoid\n",
       "}\t\n"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential()\n",
    "model:add(nn.Reshape(opt.m_frame * opt.n))\n",
    "model:add(nn.Linear(opt.m_frame * opt.n, opt.K))\n",
    "model:add(nn.Sigmoid())\n",
    "model:type(dtype)\n",
    "print(tostring(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Loss Function and Move to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crit = nn.MSECriterion()\n",
    "crit:type(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dev Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Check the Loss is Reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "It should be near 0.053291834317721\t0.038674008101225\t\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model:reset()\n",
    "\n",
    "local S = model:forward(X_dev)\n",
    "local J = crit:forward(S, Y_dev)\n",
    "print('It should be near '.. torch.dot(Y_dev, Y_dev) / opt.m_dev / opt.n, J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss_grad Function for Dev Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta, dtheta = model:getParameters()\n",
    "\n",
    "function loss_grad_dev(theta_new)\n",
    "    if theta ~= theta_new then\n",
    "        theta:copy(theta_new)\n",
    "    end\n",
    "    dtheta:zero()\n",
    "        \n",
    "    -- Forward and backward pass.\n",
    "    local S = model:forward(X_dev)\n",
    "    local J = crit:forward(S, Y_dev)\n",
    "    \n",
    "    local dS = crit:backward(S, Y_dev)\n",
    "    model:backward(X_dev, dS)\n",
    "    return J, dtheta\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Function to Compute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Check the error.\n",
    "function err(X, Y, m)\n",
    "    local S = model:forward(X)\n",
    "    \n",
    "    -- Compute the l1 dist.\n",
    "    local J = torch.norm(S - Y, 1) / m / opt.K\n",
    "    return J\n",
    "end\n",
    "\n",
    "-- Check the error for big data.\n",
    "function err_big(X, Y, m)\n",
    "    local tot_err = 0\n",
    "    local m_split = m / 5\n",
    "    local X_split = X:split(m_split, 1)\n",
    "    local Y_split = Y:split(m_split, 1)\n",
    "    for i = 1, 5 do\n",
    "        tot_err = tot_err + err(X_split[i], Y_split[i], m_split)\n",
    "    end\n",
    "    return tot_err / 5\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Funtion to Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Plot train and val figure in 1 figure.\n",
    "function plot1(name, train_plots, val_plots)\n",
    "    train_plots[1][1] = 'train'\n",
    "    val_plots[1][1] = 'val'\n",
    "    train_plots[2] = val_plots[1]\n",
    "    gnuplot.pngfigure(name)\n",
    "    gnuplot.plot(train_plots)\n",
    "    gnuplot.grid('on')\n",
    "    gnuplot.plotflush()\n",
    "    print('done')\n",
    "end\n",
    "\n",
    "-- Plot train and val figure in 2 separate figures.\n",
    "function plot2(name_train, val_name, train_plots, val_plots)\n",
    "    gnuplot.pngfigure(name_train)\n",
    "    gnuplot.plot(train_plots)\n",
    "    gnuplot.grid('on')\n",
    "    gnuplot.plotflush()\n",
    "    gnuplot.pngfigure(val_name)\n",
    "    gnuplot.plot(val_plots)\n",
    "    gnuplot.grid('on')\n",
    "    gnuplot.plotflush()\n",
    "    print('done')\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting Small Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "done\t\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- Reset the network from scratch to make a fair comparasion.\n",
    "model:reset()\n",
    "\n",
    "local T = 60\n",
    "local train_err = torch.Tensor(T)\n",
    "local val_err = torch.Tensor(T)\n",
    "\n",
    "hpara = {\n",
    "    learningRate = 3e-3,\n",
    "    weightDecay = 1e-6,\n",
    "    momentum = 0,\n",
    "    learningRateDecay = 0\n",
    "}\n",
    "\n",
    "for t = 1, T do\n",
    "    optim.sgd(loss_grad_dev, theta, hpara)\n",
    "        \n",
    "    train_err[t] = err(X_dev, Y_dev, opt.m_dev)\n",
    "    val_err[t] = err(X_val, Y_val, opt.m_val)\n",
    "end\n",
    "\n",
    "local train_plots = {}\n",
    "local val_plots = {}\n",
    "table.insert(train_plots, {'', train_err})\n",
    "table.insert(val_plots, {'', val_err})\n",
    "plot1('overfit_small_debug.png', train_plots, val_plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss_grad Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta, dtheta = model:getParameters()\n",
    "\n",
    "count = 1\n",
    "function loss_grad(theta_new)\n",
    "    if theta ~= theta_new then\n",
    "        theta:copy(theta_new)\n",
    "    end\n",
    "    dtheta:zero()\n",
    "    \n",
    "    -- Fetch the data.\n",
    "    local X = X_chunks[count]\n",
    "    local Y = Y_chunks[count]\n",
    "    \n",
    "    if count == #X_chunks then\n",
    "        count = 1\n",
    "    else\n",
    "        count = count + 1\n",
    "    end\n",
    "    \n",
    "    -- Forward and backward pass.\n",
    "    local S = model:forward(X)\n",
    "    local J = crit:forward(S, Y)\n",
    "    \n",
    "    local dS = crit:backward(S, Y)\n",
    "    model:backward(X, dS)\n",
    "    return J, dtheta\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iter = math.ceil(opt.m / opt.bs)\n",
    "\n",
    "function train(hpara, epochs, i)\n",
    "    -- Reset the network from scratch to make a fair comparasion.\n",
    "    model:reset()\n",
    "    count = 1\n",
    "    local T = epochs * iter\n",
    "    local train_err = torch.Tensor(epochs)\n",
    "    local val_err = torch.Tensor(epochs)\n",
    "\n",
    "    for t = 1, T do\n",
    "        optim.sgd(loss_grad, theta, hpara)\n",
    "    \n",
    "        if t % iter == 0 then -- Print after one epoch.            \n",
    "            train_err[t / iter] = err_big(X, Y, opt.m)\n",
    "            val_err[t / iter] = err(X_val, Y_val, opt.m_val)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    table.insert(train_plots, {'' .. i, train_err})\n",
    "    table.insert(val_plots, {'' .. i, val_err})\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start with Small $\\lambda$ and Find $\\alpha$ that Makes the Loss go Down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1\talpha:\t0.0010000000474975\t\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2\talpha:\t0.0016681009437889\t\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3\talpha:\t0.002782559255138\t\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4\talpha:\t0.0046415897086263\t\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "5\talpha:\t0.0077426359057426\t\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "6\talpha:\t0.012915498577058\t\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "7\talpha:\t0.021544348448515\t\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "8\talpha:\t0.035938140004873\t\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "9\talpha:\t0.059948425740004\t\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "10\talpha:\t0.10000000149012\t\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "done\t\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local alpha_tensor = torch.logspace(-3, -1, 10)\n",
    "train_plots = {}\n",
    "val_plots = {}\n",
    "\n",
    "for i = 1, alpha_tensor:size(1) do\n",
    "    local alpha = alpha_tensor[i]\n",
    "    print(i, 'alpha:', alpha)\n",
    "    \n",
    "    hpara = {\n",
    "        learningRate = alpha,\n",
    "        weightDecay = 1e-6,\n",
    "        momentum = 0.9,\n",
    "        learningRateDecay = 5e-7\n",
    "    }    \n",
    "    train(hpara, 3, i)\n",
    "end\n",
    "\n",
    "plot2('coarse_train.png', 'coarse_val.png', train_plots, val_plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coarse to Fine Tuning the Best $\\alpha$ and $\\lambda$ with Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1\talpha:\t0.00089058804512024\tlambda:\t8.895623087883\ttau:\t2.8716179816755e-08\t\n"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2\talpha:\t0.00064225347340107\tlambda:\t7.6392441987991\ttau:\t3.5793290563736e-07\t\n"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3\talpha:\t0.00081647369265556\tlambda:\t5.1519630551338\ttau:\t8.6773674822229e-08\t\n"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4\talpha:\t0.00066141852736473\tlambda:\t7.3674860596657\ttau:\t1.3092239195384e-06\t\n"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "5\talpha:\t0.00087683382630348\tlambda:\t5.7031597495079\ttau:\t5.1255674150276e-07\t\n"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "6\talpha:\t0.00082990938425064\tlambda:\t6.4832113981247\ttau:\t"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1.018605475708e-06\t\n"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "7\talpha:\t0.00065149796009064\tlambda:\t5.7740335762501\ttau:\t1.2500347884642e-07\t\n"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "8\talpha:\t0.00067041157186031\tlambda:\t4.7581212222576\ttau:\t7.5396920609713e-08\t\n"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "9\talpha:\t0.00097438177466393\tlambda:\t7.4036501049995\ttau:\t5.5574452470173e-08\t\n"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "10\talpha:\t0.00097258642315865\tlambda:\t4.7743104696274\ttau:\t5.2613215724391e-07\t\n"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "done\t\n"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_plots = {}\n",
    "val_plots = {}\n",
    "for times = 1, 10 do\n",
    "    local alpha = (torch.rand(1)[1] * 5 + 5) * 10^(-4)\n",
    "    local lambda = torch.rand(1)[1] * 7 + 3\n",
    "    local tau = 10 ^ (torch.rand(1)[1] * -3 - 5)\n",
    "    print(times, 'alpha:', alpha, 'lambda:', lambda, 'tau:', tau)\n",
    "    \n",
    "    hpara = {\n",
    "        learningRate = alpha,\n",
    "        weightDecay = lambda,\n",
    "        momentum = 0.9,\n",
    "        learningRateDecay = tau\n",
    "    }\n",
    "    train(hpara, 40, times)\n",
    "end\n",
    "\n",
    "plot2('fine_train.png', 'fine_val.png', train_plots, val_plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "done\t\n"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_plots = {}\n",
    "val_plots = {}\n",
    "\n",
    "hpara = {\n",
    "        learningRate = 8.3e-4,\n",
    "        weightDecay = 6.5,\n",
    "        momentum = 0.9,\n",
    "        learningRateDecay = 1.01e-6\n",
    "    }\n",
    "train(hpara, 100, '')\n",
    "\n",
    "plot1('final.png', train_plots, val_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save('model.bin', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
